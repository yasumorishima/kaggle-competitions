ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚„ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ã§æ¤œç´¢


ãƒ¡ãƒ‹ãƒ¥ãƒ¼
 æŠ•ç¨¿

è¦‹å‡ºã—ç”»åƒ
ã€æ©Ÿæ¢°å­¦ç¿’ ç¬¬6å›ã€‘LLM Classification ã§å‹‰å¼·

shogaku
shogaku
2025å¹´11æœˆ26æ—¥ 19:12

Kaggle LLM Classification Finetuning ã«æŒ‘æˆ¦ï¼šåˆå¿ƒè€…ã®å­¦ç¿’è¨˜éŒ²

Notebook

Japanese : LLM Classification
Explore and run machine learning code with Kaggle Notebooks |
www.kaggle.com



ç›®æ¬¡
Kaggle LLM Classification Finetuning ã«æŒ‘æˆ¦ï¼šåˆå¿ƒè€…ã®å­¦ç¿’è¨˜éŒ²
ã¯ã˜ã‚ã«
ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³æ¦‚è¦
ã‚¿ã‚¹ã‚¯
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
è©•ä¾¡æŒ‡æ¨™
ğŸ“¦ 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
ğŸ“‚ 2. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
ğŸ” 3. æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰

ã™ã¹ã¦è¡¨ç¤º


ã¯ã˜ã‚ã«
Kaggleåˆå¿ƒè€…ã¨ã—ã¦ã€ã€ŒLLM Classification Finetuningã€ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æŒ‘æˆ¦ã—ã¾ã—ãŸã€‚

ã“ã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã§ã¯ã€Chatbot Arenaã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã©ã®LLMã®å¿œç­”ã‚’å¥½ã‚€ã‹ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚ã“ã®è¨˜äº‹ã§ã¯ã€Notebookã®å®Ÿè£…ã®æµã‚Œã«æ²¿ã£ã¦ã€å­¦ã‚“ã ã“ã¨ã‚„å·¥å¤«ã—ãŸç‚¹ã‚’åˆå¿ƒè€…ç›®ç·šã§ã¾ã¨ã‚ã¦ã„ã¾ã™ã€‚

ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³æ¦‚è¦
ã‚¿ã‚¹ã‚¯
2ã¤ã®LLMã®å¿œç­”ã‚’æ¯”è¼ƒã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã©ã¡ã‚‰ã‚’å¥½ã‚€ã‹ã‚’äºˆæ¸¬ã™ã‚‹3ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼š

winner_model_a: ãƒ¢ãƒ‡ãƒ«AãŒå¥½ã¾ã‚ŒãŸ

winner_model_b: ãƒ¢ãƒ‡ãƒ«BãŒå¥½ã¾ã‚ŒãŸ

winner_tie: å¼•ãåˆ†ã‘

ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ
ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿: 57,477ä»¶

ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: äºˆæ¸¬å¯¾è±¡

LLM: GPT-4, Claude, Llama 2, Gemini, Mistral ãªã©70ä»¥ä¸Š

è©•ä¾¡æŒ‡æ¨™
Log Lossï¼ˆå¯¾æ•°æå¤±ï¼‰- å€¤ãŒä½ã„ã»ã©è‰¯ã„

ğŸ“¦ 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import numpy as np
import pandas as pd

# å¯è¦–åŒ–
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from wordcloud import WordCloud

# æ©Ÿæ¢°å­¦ç¿’  
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import log_loss
from sklearn.preprocessing import LabelEncoder
from sklearn.multioutput import MultiOutputClassifier
import xgboost as xgb
import lightgbm as lgb
from scipy.sparse import hstack, csr_matrix

copy
å­¦ã‚“ã ã“ã¨:

Plotlyã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚°ãƒ©ãƒ•ãŒä½œã‚Œã‚‹

XGBoost/LightGBMã¯å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã®å¼·åŠ›ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒª

MultiOutputClassifierã§è¤‡æ•°ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åŒæ™‚äºˆæ¸¬

ğŸ“‚ 2. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
train_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')
test_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')

copy
ãƒ‡ãƒ¼ã‚¿æ§‹é€ 
id: è¡ŒID

model_a, model_b: ãƒ¢ãƒ‡ãƒ«åï¼ˆâš ï¸ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¯ãªã„ï¼‰

prompt: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•

response_a, response_b: å„ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”

winner_model_a, winner_model_b, winner_tie: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ

ğŸ” 3. æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰
3.1 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†å¸ƒ
target_cols = ['winner_model_a', 'winner_model_b', 'winner_tie']
train_df['winner'] = train_df[target_cols].idxmax(axis=1)

copy
çµæœ:

Model A å‹åˆ©: 20,064ä»¶ï¼ˆ34.91%ï¼‰

Model B å‹åˆ©: 19,652ä»¶ï¼ˆ34.19%ï¼‰

å¼•ãåˆ†ã‘: 17,761ä»¶ï¼ˆ30.90%ï¼‰




å­¦ã³: ã‚¯ãƒ©ã‚¹ãŒãƒãƒ©ãƒ³ã‚¹ã—ã¦ã„ã‚‹ãŸã‚ã€ç‰¹åˆ¥ãªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ä¸è¦

3.2 ãƒ†ã‚­ã‚¹ãƒˆé•·ã®åˆ†æ
train_df['prompt_length'] = train_df['prompt'].str.len()
train_df['response_a_length'] = train_df['response_a'].str.len()
train_df['response_b_length'] = train_df['response_b'].str.len()

copy
çµ±è¨ˆ:

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¹³å‡: 369æ–‡å­—

ãƒ¬ã‚¹ãƒãƒ³ã‚¹Aå¹³å‡: 1,378æ–‡å­—

ãƒ¬ã‚¹ãƒãƒ³ã‚¹Bå¹³å‡: 1,386æ–‡å­—

ğŸ“Š ã‚°ãƒ©ãƒ•2: ãƒ†ã‚­ã‚¹ãƒˆé•·ã®åˆ†å¸ƒï¼ˆ4ã¤ã®ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰

ç”»åƒ

å­¦ã³: ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®é•·ã•ã¯é‡è¦ãªç‰¹å¾´é‡ã«ãªã‚‹

3.3 ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨é »åº¦
Top 3:

gpt-4-1106-preview: 3,700å›

gpt-3.5-turbo-0613: 3,500å›

gpt-4-0613: 3,100å›

ğŸ“Š ã‚°ãƒ©ãƒ•3: ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨é »åº¦ï¼ˆæ¨ªæ£’ã‚°ãƒ©ãƒ•ï¼‰

ç”»åƒ

å­¦ã³: GPT-4ã‚„Claudeç³»ãŒé »ç¹ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹

3.4 ãƒ¢ãƒ‡ãƒ«åˆ¥å‹ç‡åˆ†æ â­
ã“ã‚ŒãŒæœ€é‡è¦ãªåˆ†æã§ã™ï¼ å„ãƒ¢ãƒ‡ãƒ«ã®å¼·ã•ã‚’æŠŠæ¡ã—ã€å¾Œã§ç‰¹å¾´é‡ã¨ã—ã¦æ´»ç”¨ã—ã¾ã™ã€‚

# ãƒ¢ãƒ‡ãƒ«åˆ¥å‹ç‡ã®è¨ˆç®—
all_models_a = train_df.groupby('model_a')['winner_model_a'].agg(['sum', 'count'])
all_models_b = train_df.groupby('model_b')['winner_model_b'].agg(['sum', 'count'])

combined = pd.DataFrame({
    'wins': all_models_a['sum'].add(all_models_b['sum'], fill_value=0),
    'total': all_models_a['count'].add(all_models_b['count'], fill_value=0)
})
combined['win_rate'] = (combined['wins'] / combined['total'] * 100)

copy
Top 5ï¼ˆ50å›ä»¥ä¸Šå‡ºç¾ï¼‰:

gpt-4-1106-preview: 55.1%

gpt-3.5-turbo-0314: 54.6%

gpt-4-0125-preview: 51.4%

gpt-4-0314: 48.4%

claude-1: 43.9%

ğŸ“Š ã‚°ãƒ©ãƒ•4: ãƒ¢ãƒ‡ãƒ«åˆ¥å‹ç‡ï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰

ç”»åƒ

å­¦ã³: GPT-4ç³»ãŒåœ§å€’çš„ã«å¼·ã„ï¼ã“ã®æƒ…å ±ã‚’ç‰¹å¾´é‡ã«ä½¿ã†

3.5 WordCloudåˆ†æ
all_prompts = ' '.join(train_df['prompt'].astype(str).values)
wordcloud = WordCloud(
    width=1200, height=600,
    background_color='white',
    stopwords=STOPWORDS,
    max_words=100
).generate(all_prompts)

copy
ğŸ“Š ã‚°ãƒ©ãƒ•5: WordCloud

ç”»åƒ


é »å‡ºå˜èª: "write", "explain", "how", "create"ãªã©ã®ã‚¿ã‚¹ã‚¯ç³»å˜èª

ğŸ”§ 4. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
åˆè¨ˆ330å€‹ã®ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã™ã€‚

4.1 åŸºæœ¬çš„ãªç‰¹å¾´é‡ï¼ˆ26å€‹ï¼‰
def create_features(df):
    df = df.copy()
    
    # ãƒ†ã‚­ã‚¹ãƒˆé•·
    df['prompt_length'] = df['prompt'].str.len()
    df['response_a_length'] = df['response_a'].str.len()
    df['response_b_length'] = df['response_b'].str.len()
    
    # å˜èªæ•°
    df['prompt_words'] = df['prompt'].str.split().str.len()
    df['response_a_words'] = df['response_a'].str.split().str.len()
    df['response_b_words'] = df['response_b'].str.split().str.len()
    
    # å·®ã¨æ¯”ç‡ï¼ˆã‚¼ãƒ­é™¤ç®—å›é¿ã®ãŸã‚+1ï¼‰
    df['length_diff'] = df['response_a_length'] - df['response_b_length']
    df['length_ratio'] = df['response_a_length'] / (df['response_b_length'] + 1)
    df['words_diff'] = df['response_a_words'] - df['response_b_words']
    df['words_ratio'] = df['response_a_words'] / (df['response_b_words'] + 1)
    
    # å¹³å‡å˜èªé•·
    df['avg_word_length_a'] = df['response_a_length'] / (df['response_a_words'] + 1)
    df['avg_word_length_b'] = df['response_b_length'] / (df['response_b_words'] + 1)
    
    # ãƒ†ã‚­ã‚¹ãƒˆç‰¹æ€§
    df['punctuation_a'] = df['response_a'].str.count(r'[.,!?;:]')
    df['punctuation_b'] = df['response_b'].str.count(r'[.,!?;:]')
    df['uppercase_a'] = df['response_a'].str.count(r'[A-Z]')
    df['uppercase_b'] = df['response_b'].str.count(r'[A-Z]')
    df['digits_a'] = df['response_a'].str.count(r'\d')
    df['digits_b'] = df['response_b'].str.count(r'\d')
    df['newlines_a'] = df['response_a'].str.count(r'\n')
    df['newlines_b'] = df['response_b'].str.count(r'\n')
    
    # æ§‹é€ çš„ç‰¹å¾´ï¼ˆã‚³ãƒ¼ãƒ‰ã€ãƒªã‚¹ãƒˆã®æ¤œå‡ºï¼‰
    df['has_code_a'] = df['response_a'].str.contains(r'```', regex=True).astype(int)
    df['has_code_b'] = df['response_b'].str.contains(r'```', regex=True).astype(int)
    df['has_list_a'] = df['response_a'].str.contains(r'\n\s*[â€¢\-\*\d+\.]', regex=True).astype(int)
    df['has_list_b'] = df['response_b'].str.contains(r'\n\s*[â€¢\-\*\d+\.]', regex=True).astype(int)
    
    return df

copy
é‡è¦ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯:

+ 1ã§ã‚¼ãƒ­é™¤ç®—ã‚’å›é¿

æ­£è¦è¡¨ç¾ã§ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚„ãƒªã‚¹ãƒˆã‚’æ¤œå‡º

æ§‹é€ åŒ–ã•ã‚ŒãŸå¿œç­”ï¼ˆã‚³ãƒ¼ãƒ‰ã€ãƒªã‚¹ãƒˆï¼‰ã¯å¥½ã¾ã‚Œã‚‹å‚¾å‘

4.2 ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ç‰¹å¾´é‡åŒ–ï¼ˆ6å€‹ï¼‰ â­
æœ€ã‚‚åŠ¹æœçš„ãªç‰¹å¾´é‡ã§ã™ï¼

model_win_rates = combined['win_rate'].to_dict()

def add_model_features(df):
    df = df.copy()
    
    if 'model_a' in df.columns and 'model_b' in df.columns:
        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿: å®Ÿéš›ã®ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ä½¿ç”¨
        df['model_a_win_rate'] = df['model_a'].map(model_win_rates).fillna(35.0)
        df['model_b_win_rate'] = df['model_b'].map(model_win_rates).fillna(35.0)
        df['win_rate_diff'] = df['model_a_win_rate'] - df['model_b_win_rate']
        
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã®åˆ†é¡
        def extract_family(m):
            if 'gpt-4' in m: return 'gpt4'
            elif 'gpt-3.5' in m: return 'gpt35'
            elif 'claude' in m: return 'claude'
            elif 'llama' in m: return 'llama'
            elif 'mistral' in m or 'mixtral' in m: return 'mistral'
            else: return 'other'
        
        df['model_a_family'] = df['model_a'].apply(extract_family)
        df['model_b_family'] = df['model_b'].apply(extract_family)
        df['same_family'] = (df['model_a_family'] == df['model_b_family']).astype(int)
    else:
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        df['model_a_win_rate'] = 35.0
        df['model_b_win_rate'] = 35.0
        df['win_rate_diff'] = 0.0
        df['model_a_family'] = 'other'
        df['model_b_family'] = 'other'
        df['same_family'] = 0
    
    return df

copy
âš ï¸ é‡è¦ãªæ³¨æ„ç‚¹: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ã¯model_a/model_bæƒ…å ±ãŒãªã„ãŸã‚ã€æ¡ä»¶åˆ†å²ãŒå¿…è¦ã§ã™ã€‚

å­¦ã³: ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ï¼ˆã©ã®ãƒ¢ãƒ‡ãƒ«ãŒå¼·ã„ã‹ï¼‰ã‚’ç‰¹å¾´é‡ã«çµ„ã¿è¾¼ã‚€å¨åŠ›

4.3 TF-IDFç‰¹å¾´é‡ï¼ˆ300å€‹ï¼‰
ãƒ†ã‚­ã‚¹ãƒˆã®å†…å®¹ã‚’æ•°å€¤åŒ–ã—ã¾ã™ã€‚

tfidf_prompt = TfidfVectorizer(
    max_features=100,
    stop_words='english',
    ngram_range=(1, 2),  # ãƒ¦ãƒ‹ã‚°ãƒ©ãƒ ã¨ãƒã‚¤ã‚°ãƒ©ãƒ 
    min_df=5             # æœ€ä½5å›å‡ºç¾
)

prompt_tfidf_train = tfidf_prompt.fit_transform(train_df['prompt'])
prompt_tfidf_test = tfidf_prompt.transform(test_df['prompt'])

# response_a ã¨ response_b ã‚‚åŒæ§˜ã«å‡¦ç†

copy
ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£èª¬:

max_features=100: ä¸Šä½100å˜èª

ngram_range=(1, 2): å˜èªã¨ãƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆ2å˜èªï¼‰ã®ä¸¡æ–¹

min_df=5: ç¨€ãªå˜èªã‚’é™¤å¤–

å­¦ã³: TF-IDFã¯ãƒ†ã‚­ã‚¹ãƒˆã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹æ¨™æº–æ‰‹æ³•

4.4 ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã®çµåˆ
ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®ãŸã‚ã€ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã¨ã—ã¦çµåˆï¼š

X_train = hstack([
    csr_matrix(X_train_num.values),  # æ•°å€¤ç‰¹å¾´é‡
    prompt_tfidf_train,               # ã‚¹ãƒ‘ãƒ¼ã‚¹
    response_a_tfidf_train,           # ã‚¹ãƒ‘ãƒ¼ã‚¹
    response_b_tfidf_train            # ã‚¹ãƒ‘ãƒ¼ã‚¹
])

copy
æœ€çµ‚ç‰¹å¾´é‡: (57,477, 330)

å­¦ã³: ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã§30ä¸‡æ¬¡å…ƒã§ã‚‚ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚ˆãå‡¦ç†

ğŸ¯ 5. ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
4ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

5.1 ãƒ‡ãƒ¼ã‚¿åˆ†å‰²
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42, 
    stratify=train_df['winner']  # å±¤åŒ–æŠ½å‡º
)

copy
å­¦ã³: stratifyã§ã‚¯ãƒ©ã‚¹æ¯”ç‡ã‚’ä¿ã£ãŸã¾ã¾åˆ†å‰²

5.2 ãƒ¢ãƒ‡ãƒ«1: ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
lr_model = MultiOutputClassifier(
    LogisticRegression(max_iter=1000, random_state=42, solver='saga', C=1.0)
)
lr_model.fit(X_train_split, y_train_split)

# äºˆæ¸¬
y_val_pred_lr = lr_model.predict_proba(X_val_split)
y_val_pred_lr_array = np.column_stack([pred[:, 1] for pred in y_val_pred_lr])

# è©•ä¾¡
val_logloss_lr = log_loss(y_val_split, y_val_pred_lr_array)

copy
çµæœ: Log Loss 1.0457

å­¦ã³: MultiOutputClassifierã§3ã¤ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’åŒæ™‚äºˆæ¸¬

5.3 ãƒ¢ãƒ‡ãƒ«2: XGBoost
xgb_model = MultiOutputClassifier(
    xgb.XGBClassifier(
        n_estimators=200,    # ãƒ„ãƒªãƒ¼ã®æ•°
        max_depth=6,         # ãƒ„ãƒªãƒ¼ã®æ·±ã•
        learning_rate=0.1,   # å­¦ç¿’ç‡
        random_state=42,
        tree_method='hist'   # é«˜é€ŸåŒ–
    )
)
xgb_model.fit(X_train_split, y_train_split)

copy
çµæœ: Log Loss 1.0003 â­

å­¦ã³: XGBoostã¯è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã«éå¸¸ã«å¼·åŠ›

5.4 ãƒ¢ãƒ‡ãƒ«3: LightGBM
lgb_model = MultiOutputClassifier(
    lgb.LGBMClassifier(
        n_estimators=200,
        max_depth=6,
        learning_rate=0.1,
        random_state=42,
        verbose=-1
    )
)
lgb_model.fit(X_train_split, y_train_split)

copy
çµæœ: Log Loss 1.0013

å­¦ã³: LightGBMã‚‚XGBoostä¸¦ã¿ã®æ€§èƒ½

5.5 ãƒ¢ãƒ‡ãƒ«4: ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ
rf_model = MultiOutputClassifier(
    RandomForestClassifier(
        n_estimators=100,
        max_depth=10,
        random_state=42,
        n_jobs=-1  # å…¨CPUã‚³ã‚¢ã‚’ä½¿ç”¨
    )
)
rf_model.fit(X_train_split, y_train_split)

copy
çµæœ: Log Loss 1.0180

5.6 ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ¯”è¼ƒ
ç”»åƒ
ğŸ“Š ã‚°ãƒ©ãƒ•6: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒï¼ˆæ£’ã‚°ãƒ©ãƒ•ï¼‰

![ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ](./images/model_comparison.png)

copy
å­¦ã³: å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ï¼ˆXGBoost/LightGBMï¼‰ãŒåœ§å€’çš„

5.7 ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
# é‡ã¿ä»˜ãå¹³å‡
y_val_pred_ensemble = (
    y_val_pred_lr_array * 0.2 + 
    y_val_pred_xgb_array * 0.3 + 
    y_val_pred_lgb_array * 0.3 + 
    y_val_pred_rf_array * 0.2
)

val_logloss_ensemble = log_loss(y_val_split, y_val_pred_ensemble)

copy
çµæœ: Log Loss 1.0028

å­¦ã³: ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã¯å®‰å®šã™ã‚‹ãŒã€ä»Šå›ã¯XGBoostå˜ä½“ãŒæœ€è‰¯

5.8 å…¨ãƒ‡ãƒ¼ã‚¿ã§å†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
æœ€çµ‚æå‡ºç”¨ã«ã€å…¨ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã§ãƒ¢ãƒ‡ãƒ«ã‚’å†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚

lr_final = MultiOutputClassifier(LogisticRegression(...))
lr_final.fit(X_train, y_train)

xgb_final = MultiOutputClassifier(xgb.XGBClassifier(...))
xgb_final.fit(X_train, y_train)

lgb_final = MultiOutputClassifier(lgb.LGBMClassifier(...))
lgb_final.fit(X_train, y_train)

rf_final = MultiOutputClassifier(RandomForestClassifier(...))
rf_final.fit(X_train, y_train)

copy
å­¦ã³: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¯è©•ä¾¡ç”¨ã€æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ã¯å…¨ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’

ğŸ“¤ 6. äºˆæ¸¬ã¨æå‡º
# å„ãƒ¢ãƒ‡ãƒ«ã§äºˆæ¸¬
y_test_pred_lr = lr_final.predict_proba(X_test)
y_test_pred_xgb = xgb_final.predict_proba(X_test)
y_test_pred_lgb = lgb_final.predict_proba(X_test)
y_test_pred_rf = rf_final.predict_proba(X_test)

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
y_test_pred_array = (
    y_test_pred_lr_array * 0.2 + 
    y_test_pred_xgb_array * 0.3 + 
    y_test_pred_lgb_array * 0.3 + 
    y_test_pred_rf_array * 0.2
)

# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
submission = pd.DataFrame({
    'id': test_df['id'],
    'winner_model_a': y_test_pred_array[:, 0],
    'winner_model_b': y_test_pred_array[:, 1],
    'winner_tie': y_test_pred_array[:, 2]
})

submission.to_csv('submission.csv', index=False)

copy
æœ€çµ‚çµæœ:

æ¤œè¨¼ Log Loss: 1.0003ï¼ˆXGBoostï¼‰

å®Ÿéš›ã®æå‡ºã‚¹ã‚³ã‚¢: 1.05812

å­¦ã‚“ã ã“ã¨ãƒ»æ°—ã¥ã
âœ… ã†ã¾ãã„ã£ãŸã“ã¨
ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®æ´»ç”¨

å„ãƒ¢ãƒ‡ãƒ«ã®å‹ç‡ã‚’ç‰¹å¾´é‡ã«çµ„ã¿è¾¼ã‚€ã“ã¨ã§å¤§å¹…ã«æ”¹å–„

ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã®é‡è¦æ€§ã‚’å®Ÿæ„Ÿ

å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã®å¨åŠ›

XGBoost/LightGBMãŒä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚ˆã‚Šåœ§å€’çš„ã«å„ªç§€

ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚‚è‰¯å¥½ãªçµæœ

å¤šæ§˜ãªç‰¹å¾´é‡

åŸºæœ¬çµ±è¨ˆï¼ˆ26ï¼‰+ ãƒ¢ãƒ‡ãƒ«æƒ…å ±ï¼ˆ6ï¼‰+ TF-IDFï¼ˆ300ï¼‰

æ§‹é€ çš„ç‰¹å¾´ï¼ˆã‚³ãƒ¼ãƒ‰ã€ãƒªã‚¹ãƒˆï¼‰ã®æ¤œå‡ºã‚‚æœ‰åŠ¹

å¯è¦–åŒ–ã®é‡è¦æ€§

EDAã§ä»®èª¬ã‚’ç«‹ã¦ã€ãã‚Œã‚’ç‰¹å¾´é‡ã«åæ˜ 

Plotlyã§ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚°ãƒ©ãƒ•ãŒç°¡å˜ã«ä½œã‚Œã‚‹

âŒ é›£ã—ã‹ã£ãŸã“ã¨
ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ‰±ã„

model_a/model_bæƒ…å ±ãŒãªã„ã“ã¨ã®å¯¾å¿œ

æ¡ä»¶åˆ†å²ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š

éå­¦ç¿’ã®å…†å€™

æ¤œè¨¼: 1.0003

å®Ÿéš›: 1.05812

ç´„5%ã®åŠ£åŒ–ãŒè¦‹ã‚‰ã‚Œã‚‹

ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã®æ‰±ã„

hstackã¨csr_matrixã®ä½¿ã„æ–¹ã«æ…£ã‚ŒãŒå¿…è¦

ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¯è‰¯ã„ãŒã€ãƒ‡ãƒãƒƒã‚°ãŒé›£ã—ã„

ä»Šå¾Œã®æ”¹å–„æ¡ˆ
1. Cross-Validation
ç¾åœ¨ã¯å˜ç´”ãªtrain/validåˆ†å‰²ã€‚ã‚ˆã‚Šå®‰å®šã—ãŸè©•ä¾¡ã®ãŸã‚5-fold CVã‚’å°å…¥ï¼š

from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
    # å„foldã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
    # å¹³å‡ã‚¹ã‚³ã‚¢ã§è©•ä¾¡

copy
2. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
Optunaã§è‡ªå‹•æœ€é©åŒ–ï¼š

import optuna

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 500),
        'max_depth': trial.suggest_int('max_depth', 4, 10),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3)
    }
    model = xgb.XGBClassifier(**params)
    # å­¦ç¿’ã¨è©•ä¾¡
    return log_loss

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)

copy
3. ã‚ˆã‚Šé«˜åº¦ãªç‰¹å¾´é‡
ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æ: TextBlobã‚„VADERã§æ„Ÿæƒ…ã‚¹ã‚³ã‚¢

å¯èª­æ€§æŒ‡æ¨™: Flesch Reading Ease Score

ãƒ†ã‚­ã‚¹ãƒˆé¡ä¼¼åº¦: response_aã¨response_bã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦

ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£èªè­˜: äººåã€åœ°åãªã©ã®å›ºæœ‰è¡¨ç¾

4. æ·±å±¤å­¦ç¿’ã‚¢ãƒ—ãƒ­ãƒ¼ãƒï¼ˆè¦GPUï¼‰
from transformers import AutoTokenizer, AutoModel

tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')
model = AutoModel.from_pretrained('microsoft/deberta-v3-base')

# ãƒ†ã‚­ã‚¹ãƒˆã‚’embeddingã«å¤‰æ›
embeddings = model(**tokenizer(text, return_tensors='pt'))

copy
ã¾ã¨ã‚
åˆå¿ƒè€…ã¨ã—ã¦åˆã‚ã¦å–ã‚Šçµ„ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã§ã—ãŸãŒã€å¤šãã®ã“ã¨ã‚’å­¦ã¹ã¾ã—ãŸï¼š

ä¸»ãªå­¦ã³
ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®é‡è¦æ€§

å˜ç´”ãªçµ±è¨ˆé‡ã‹ã‚‰ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã¾ã§ã€æ§˜ã€…ãªè§’åº¦ã‹ã‚‰ç‰¹å¾´é‡ã‚’ä½œæˆ

ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®æ´»ç”¨ãŒæœ€ã‚‚åŠ¹æœçš„ã ã£ãŸ

é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«é¸æŠ

XGBoost/LightGBMã¯è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã«éå¸¸ã«å¼·åŠ›

ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã¯å¿…ãšã—ã‚‚æœ€è‰¯ã¨ã¯é™ã‚‰ãªã„

å¯è¦–åŒ–ã«ã‚ˆã‚‹ç†è§£

EDAã§ä»®èª¬ã‚’ç«‹ã¦ã€ãã‚Œã‚’ç‰¹å¾´é‡ã«åæ˜ 

Plotlyã§ç¾ã—ã„ã‚°ãƒ©ãƒ•ãŒç°¡å˜ã«ä½œã‚Œã‚‹

å®Ÿè£…ã‚¹ã‚­ãƒ«

ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ã®æ‰±ã„

MultiOutputClassifierã®ä½¿ç”¨

åŠ¹ç‡çš„ãªã‚³ãƒ¼ãƒ‰æ§‹é€ åŒ–

æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
ãƒ‡ãƒ¼ã‚¿å‡¦ç†: pandas, numpy

å¯è¦–åŒ–: matplotlib, seaborn, plotly, wordcloud

æ©Ÿæ¢°å­¦ç¿’: scikit-learn, xgboost, lightgbm

ç’°å¢ƒ: Kaggle Notebookï¼ˆCPUã€Internet OFFï¼‰

æ¬¡å›ã¸ã®èª²é¡Œ
ã“ã®çµŒé¨“ã‚’æ´»ã‹ã—ã¦ã€æ¬¡ã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã§ã¯ï¼š

Cross-Validationã®å®Ÿè£…

ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°

æ·±å±¤å­¦ç¿’ã¸ã®æŒ‘æˆ¦

ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°

ã«æŒ‘æˆ¦ã—ã¦ã„ããŸã„ã¨æ€ã„ã¾ã™ï¼

å‚è€ƒãƒªã‚½ãƒ¼ã‚¹
Chatbot Arena Paper

LMSYS Org

Kaggle Competition

scikit-learn Documentation

XGBoost Documentation

LightGBM Documentation

æœ€å¾Œã«: ã“ã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’é€šã˜ã¦ã€æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿè·µçš„ãªã‚¹ã‚­ãƒ«ã‚’å¤šãå­¦ã¶ã“ã¨ãŒã§ãã¾ã—ãŸã€‚åˆå¿ƒè€…ã®æ–¹ã«ã¨ã£ã¦ã€ã“ã®è¨˜äº‹ãŒå‚è€ƒã«ãªã‚Œã°å¹¸ã„ã§ã™ï¼

#kaggle
#ç‰¹å¾´é‡
#XGBoost





shogaku
shogaku
GASã€VBAã€Pythonã‚’ä½¿ã£ã¦æ¥­å‹™æ”¹å–„ã¨ã‹ã—ã¦ã„ã¾ã™ã€‚å‚™å¿˜éŒ²ã‚„å‹‰å¼·ãƒ¡ãƒ¢ã‚’è¨˜è¼‰ã—ã¦ã„ã¾ã™ã€‚



noteãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
note pro
ã‚ˆãã‚ã‚‹è³ªå•ãƒ»noteã®ä½¿ã„æ–¹
ãƒ—ãƒ©ã‚¤ãƒã‚·ãƒ¼
ã‚¯ãƒªã‚¨ã‚¤ã‚¿ãƒ¼ã¸ã®ãŠå•ã„åˆã‚ã›
ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯
ã”åˆ©ç”¨è¦ç´„
é€šå¸¸ãƒã‚¤ãƒ³ãƒˆåˆ©ç”¨ç‰¹ç´„
åŠ ç›Ÿåº—è¦ç´„
è³‡â¾¦æ±ºæ¸ˆæ³•ã«åŸºã¥ãè¡¨â½°
ç‰¹å•†æ³•è¡¨è¨˜
æŠ•è³‡æƒ…å ±ã®å…è²¬äº‹é …
ã€æ©Ÿæ¢°å­¦ç¿’ ç¬¬6å›ã€‘LLM Classification ã§å‹‰å¼·ï½œshogaku
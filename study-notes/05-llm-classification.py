#!/usr/bin/env python
# coding: utf-8

# # ğŸ¤– LLM Classification: Chatbot Arena Human Preference Prediction
# 
# ## ğŸ“‹ ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³æ¦‚è¦
# 
# ã“ã®Notebookã§ã¯ã€**Chatbot Arena**ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã©ã®LLMã®å¿œç­”ã‚’å¥½ã‚€ã‹ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
# 
# ### ğŸ¯ ç›®çš„
# - 2ã¤ã®å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«(LLM)ã®å¿œç­”ã‚’æ¯”è¼ƒã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã‚’äºˆæ¸¬
# - 3ã¤ã®ã‚¯ãƒ©ã‚¹åˆ†é¡: `winner_model_a`, `winner_model_b`, `winner_tie`
# 
# ### ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ  
# - **ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿**: 55,000ä»¶ä»¥ä¸Šã®å®Ÿéš›ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å¯¾è©±
# - **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿**: 25,000ä»¶ã®äºˆæ¸¬å¯¾è±¡ã‚µãƒ³ãƒ—ãƒ«
# - **LLM**: GPT-4, Claude, Llama 2, Gemini, Mistral ãªã©70ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«

# ---
# ## ğŸ“¦ 1. ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

# In[1]:


import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

# å¯è¦–åŒ–
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from wordcloud import WordCloud, STOPWORDS

# æ©Ÿæ¢°å­¦ç¿’  
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import log_loss
from sklearn.preprocessing import LabelEncoder
from sklearn.multioutput import MultiOutputClassifier
import xgboost as xgb
import lightgbm as lgb
from scipy.sparse import hstack, csr_matrix

plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette('husl')
plt.rcParams['figure.figsize'] = (12, 6)

print('âœ… ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå®Œäº†!')


# ---
# ## ğŸ“‚ 2. ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿

# In[2]:


train_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/train.csv')
test_df = pd.read_csv('/kaggle/input/llm-classification-finetuning/test.csv')

print(f'ğŸ“Š ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿: {train_df.shape[0]:,} è¡Œ Ã— {train_df.shape[1]} åˆ—')
print(f'ğŸ“Š ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_df.shape[0]:,} è¡Œ Ã— {test_df.shape[1]} åˆ—')
print('\nğŸ“‹ æœ€åˆã®3è¡Œ:')
display(train_df.head(3))


# ---
# ## ğŸ” 3. æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰
# 
# ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã‚’å¯è¦–åŒ–ã—ã¦ç†è§£ã—ã¾ã™ã€‚

# ### 3.1 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†å¸ƒ

# In[3]:


target_cols = ['winner_model_a', 'winner_model_b', 'winner_tie']
train_df['winner'] = train_df[target_cols].idxmax(axis=1)

winner_counts = train_df['winner'].value_counts()
winner_pct = (train_df['winner'].value_counts(normalize=True) * 100)

print('ğŸ¯ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã®åˆ†å¸ƒ:')
for col in target_cols:
    print(f'   {col}: {winner_counts[col]:,} ({winner_pct[col]:.2f}%)')


# In[4]:


# å¯è¦–åŒ–: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã®åˆ†å¸ƒ
fig = make_subplots(
    rows=1, cols=2,
    subplot_titles=('ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã®åˆ†å¸ƒ (ä»¶æ•°)', 'ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚¯ãƒ©ã‚¹ã®åˆ†å¸ƒ (å‰²åˆ)'),
    specs=[[{'type':'bar'}, {'type':'pie'}]]
)

fig.add_trace(go.Bar(
    x=['Model Aå‹åˆ©', 'Model Bå‹åˆ©', 'å¼•ãåˆ†ã‘'],
    y=[winner_counts['winner_model_a'], winner_counts['winner_model_b'], winner_counts['winner_tie']],
    marker_color=['#FF6B6B', '#4ECDC4', '#45B7D1'],
    text=[f'{v:,}' for v in [winner_counts['winner_model_a'], winner_counts['winner_model_b'], winner_counts['winner_tie']]],
    textposition='outside'
), row=1, col=1)

fig.add_trace(go.Pie(
    labels=['Model Aå‹åˆ©', 'Model Bå‹åˆ©', 'å¼•ãåˆ†ã‘'],
    values=[winner_counts['winner_model_a'], winner_counts['winner_model_b'], winner_counts['winner_tie']],
    marker_colors=['#FF6B6B', '#4ECDC4', '#45B7D1'],
    hole=0.3
), row=1, col=2)

fig.update_layout(title_text='<b>ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†å¸ƒåˆ†æ</b>', title_font_size=20, height=400, showlegend=False)
fig.show()


# ### 3.2 ãƒ†ã‚­ã‚¹ãƒˆé•·ã®åˆ†æ
# 
# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®é•·ã•ã¯é‡è¦ãªç‰¹å¾´é‡ã«ãªã‚Šã¾ã™ã€‚

# In[5]:


train_df['prompt_length'] = train_df['prompt'].str.len()
train_df['response_a_length'] = train_df['response_a'].str.len()
train_df['response_b_length'] = train_df['response_b'].str.len()
train_df['response_diff'] = train_df['response_a_length'] - train_df['response_b_length']

print('ğŸ“ ãƒ†ã‚­ã‚¹ãƒˆé•·ã®çµ±è¨ˆ:')
print(f'\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå¹³å‡: {train_df["prompt_length"].mean():.1f} æ–‡å­—')
print(f'ãƒ¬ã‚¹ãƒãƒ³ã‚¹Aå¹³å‡: {train_df["response_a_length"].mean():.1f} æ–‡å­—')
print(f'ãƒ¬ã‚¹ãƒãƒ³ã‚¹Bå¹³å‡: {train_df["response_b_length"].mean():.1f} æ–‡å­—')


# In[6]:


# å¯è¦–åŒ–: ãƒ†ã‚­ã‚¹ãƒˆé•·ã®åˆ†å¸ƒ
fig = make_subplots(rows=2, cols=2, subplot_titles=('ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—æ•°', 'ãƒ¬ã‚¹ãƒãƒ³ã‚¹Aæ–‡å­—æ•°', 'ãƒ¬ã‚¹ãƒãƒ³ã‚¹Bæ–‡å­—æ•°', 'ãƒ¬ã‚¹ãƒãƒ³ã‚¹é•·ã®å·®'), vertical_spacing=0.12)

fig.add_trace(go.Histogram(x=train_df['prompt_length'], nbinsx=50, marker_color='#FF6B6B'), row=1, col=1)
fig.add_trace(go.Histogram(x=train_df['response_a_length'], nbinsx=50, marker_color='#4ECDC4'), row=1, col=2)
fig.add_trace(go.Histogram(x=train_df['response_b_length'], nbinsx=50, marker_color='#45B7D1'), row=2, col=1)
fig.add_trace(go.Histogram(x=train_df['response_diff'], nbinsx=50, marker_color='#FFA07A'), row=2, col=2)

fig.update_layout(title_text='<b>ãƒ†ã‚­ã‚¹ãƒˆé•·ã®åˆ†å¸ƒåˆ†æ</b>', title_font_size=20, height=700, showlegend=False)
fig.update_xaxes(title_text='æ–‡å­—æ•°', row=1, col=1)
fig.update_xaxes(title_text='æ–‡å­—æ•°', row=1, col=2)
fig.update_xaxes(title_text='æ–‡å­—æ•°', row=2, col=1)
fig.update_xaxes(title_text='æ–‡å­—æ•°ã®å·®', row=2, col=2)
fig.show()


# ### 3.3 ãƒ¢ãƒ‡ãƒ«ã®ä½¿ç”¨é »åº¦

# In[7]:


model_a_counts = train_df['model_a'].value_counts().head(15)
model_b_counts = train_df['model_b'].value_counts().head(15)

fig = make_subplots(rows=1, cols=2, subplot_titles=('Model A é »åº¦ Top 15', 'Model B é »åº¦ Top 15'))

fig.add_trace(go.Bar(y=model_a_counts.index, x=model_a_counts.values, orientation='h', marker_color='#FF6B6B'), row=1, col=1)
fig.add_trace(go.Bar(y=model_b_counts.index, x=model_b_counts.values, orientation='h', marker_color='#4ECDC4'), row=1, col=2)

fig.update_layout(title_text='<b>ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã®é »åº¦åˆ†æ</b>', title_font_size=20, height=600, showlegend=False)
fig.update_xaxes(title_text='å‡ºç¾å›æ•°', row=1, col=1)
fig.update_xaxes(title_text='å‡ºç¾å›æ•°', row=1, col=2)
fig.show()


# ### 3.4 ãƒ¢ãƒ‡ãƒ«åˆ¥å‹ç‡åˆ†æ
# 
# å„ãƒ¢ãƒ‡ãƒ«ã®å¼·ã•ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚ã“ã‚Œã¯å¾Œã§ç‰¹å¾´é‡ã¨ã—ã¦ä½¿ã„ã¾ã™ï¼

# In[8]:


# ãƒ¢ãƒ‡ãƒ«åˆ¥å‹ç‡ã®è¨ˆç®—
all_models_a = train_df.groupby('model_a')['winner_model_a'].agg(['sum', 'count'])
all_models_b = train_df.groupby('model_b')['winner_model_b'].agg(['sum', 'count'])

combined = pd.DataFrame({
    'wins': all_models_a['sum'].add(all_models_b['sum'], fill_value=0),
    'total': all_models_a['count'].add(all_models_b['count'], fill_value=0)
})
combined['win_rate'] = (combined['wins'] / combined['total'] * 100).fillna(0)
combined = combined[combined['total'] >= 50].sort_values('win_rate', ascending=False).head(15)

print('ğŸ† ãƒ¢ãƒ‡ãƒ«åˆ¥ç·åˆå‹ç‡ Top 15 (50å›ä»¥ä¸Šå‡ºç¾):')
print(combined[['wins', 'total', 'win_rate']].round(2))


# In[9]:


fig = go.Figure()
fig.add_trace(go.Bar(
    x=combined.index, y=combined['win_rate'],
    marker_color='#45B7D1',
    text=[f'{v:.1f}%' for v in combined['win_rate']],
    textposition='outside'
))

fig.update_layout(
    title='<b>ãƒ¢ãƒ‡ãƒ«åˆ¥ç·åˆå‹ç‡ Top 15</b><br><sub>(50å›ä»¥ä¸Šå‡ºç¾ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ã¿)</sub>',
    title_font_size=20, xaxis_title='ãƒ¢ãƒ‡ãƒ«å', yaxis_title='å‹ç‡ (%)',
    height=500, xaxis_tickangle=-45
)
fig.show()


# ### 3.5 WordCloudåˆ†æ

# In[10]:


all_prompts = ' '.join(train_df['prompt'].astype(str).values)
stopwords = set(STOPWORDS)
stopwords.update(['will', 'can', 'one', 'use', 'make', 'get', 'give', 'tell'])

wordcloud = WordCloud(width=1200, height=600, background_color='white', stopwords=stopwords, colormap='viridis', max_words=100).generate(all_prompts)

plt.figure(figsize=(16, 8))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é »å‡ºå˜èª WordCloud', fontsize=20, fontweight='bold', pad=20)
plt.tight_layout()
plt.show()


# ---
# ## ğŸ”§ 4. ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°
# 
# 330å€‹ã®ç‰¹å¾´é‡ã‚’ä½œæˆã—ã¾ã™ã€‚

# In[11]:


def create_features(df):
    df = df.copy()

    # åŸºæœ¬çµ±è¨ˆç‰¹å¾´é‡
    df['prompt_length'] = df['prompt'].str.len()
    df['response_a_length'] = df['response_a'].str.len()
    df['response_b_length'] = df['response_b'].str.len()
    df['prompt_words'] = df['prompt'].str.split().str.len()
    df['response_a_words'] = df['response_a'].str.split().str.len()
    df['response_b_words'] = df['response_b'].str.split().str.len()

    # å·®ã¨æ¯”ç‡
    df['length_diff'] = df['response_a_length'] - df['response_b_length']
    df['length_ratio'] = df['response_a_length'] / (df['response_b_length'] + 1)
    df['words_diff'] = df['response_a_words'] - df['response_b_words']
    df['words_ratio'] = df['response_a_words'] / (df['response_b_words'] + 1)
    df['avg_word_length_a'] = df['response_a_length'] / (df['response_a_words'] + 1)
    df['avg_word_length_b'] = df['response_b_length'] / (df['response_b_words'] + 1)

    # ãƒ†ã‚­ã‚¹ãƒˆç‰¹æ€§
    df['punctuation_a'] = df['response_a'].str.count(r'[.,!?;:]')
    df['punctuation_b'] = df['response_b'].str.count(r'[.,!?;:]')
    df['uppercase_a'] = df['response_a'].str.count(r'[A-Z]')
    df['uppercase_b'] = df['response_b'].str.count(r'[A-Z]')
    df['digits_a'] = df['response_a'].str.count(r'\\d')
    df['digits_b'] = df['response_b'].str.count(r'\\d')
    df['newlines_a'] = df['response_a'].str.count(r'\\n')
    df['newlines_b'] = df['response_b'].str.count(r'\\n')
    df['has_code_a'] = df['response_a'].str.contains(r'```', regex=True).astype(int)
    df['has_code_b'] = df['response_b'].str.contains(r'```', regex=True).astype(int)
    df['has_list_a'] = df['response_a'].str.contains(r'\\n\\s*[â€¢\\-\\*\\d+\\.]', regex=True).astype(int)
    df['has_list_b'] = df['response_b'].str.contains(r'\\n\\s*[â€¢\\-\\*\\d+\\.]', regex=True).astype(int)

    return df

print('ğŸ”§ åŸºæœ¬ç‰¹å¾´é‡ã‚’ä½œæˆä¸­...')
train_featured = create_features(train_df)
test_featured = create_features(test_df)
print(f'âœ… åŸºæœ¬ç‰¹å¾´é‡ä½œæˆå®Œäº†!')


# ### 4.2 ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã®ç‰¹å¾´é‡åŒ–
# 
# â­ **ã“ã‚ŒãŒæœ€ã‚‚åŠ¹æœçš„ãªç‰¹å¾´é‡ã§ã™ï¼**
# 
# ãƒ¢ãƒ‡ãƒ«ã®å‹ç‡æƒ…å ±ã‚’ç‰¹å¾´é‡ã¨ã—ã¦æ´»ç”¨ã—ã¾ã™ã€‚

# In[12]:


print('ğŸ¤– ãƒ¢ãƒ‡ãƒ«æƒ…å ±ã‚’ç‰¹å¾´é‡åŒ–ä¸­...')

model_win_rates = combined['win_rate'].to_dict()

def add_model_features(df):
    df = df.copy()

    if 'model_a' in df.columns and 'model_b' in df.columns:
        df['model_a_win_rate'] = df['model_a'].map(model_win_rates).fillna(35.0)
        df['model_b_win_rate'] = df['model_b'].map(model_win_rates).fillna(35.0)
        df['win_rate_diff'] = df['model_a_win_rate'] - df['model_b_win_rate']

        def extract_family(m):
            if 'gpt-4' in m: return 'gpt4'
            elif 'gpt-3.5' in m: return 'gpt35'
            elif 'claude' in m: return 'claude'
            elif 'llama' in m: return 'llama'
            elif 'mistral' in m or 'mixtral' in m: return 'mistral'
            elif 'gemini' in m: return 'gemini'
            elif 'vicuna' in m: return 'vicuna'
            else: return 'other'

        df['model_a_family'] = df['model_a'].apply(extract_family)
        df['model_b_family'] = df['model_b'].apply(extract_family)
        df['same_family'] = (df['model_a_family'] == df['model_b_family']).astype(int)
    else:
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”¨ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
        df['model_a_win_rate'] = 35.0
        df['model_b_win_rate'] = 35.0
        df['win_rate_diff'] = 0.0
        df['model_a_family'] = 'other'
        df['model_b_family'] = 'other'
        df['same_family'] = 0

    return df

train_featured = add_model_features(train_featured)
test_featured = add_model_features(test_featured)

# ãƒ©ãƒ™ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
le_family_a = LabelEncoder()
le_family_b = LabelEncoder()
all_families_a = pd.concat([train_featured['model_a_family'], test_featured['model_a_family']])
all_families_b = pd.concat([train_featured['model_b_family'], test_featured['model_b_family']])
le_family_a.fit(all_families_a)
le_family_b.fit(all_families_b)

train_featured['model_a_family_encoded'] = le_family_a.transform(train_featured['model_a_family'])
train_featured['model_b_family_encoded'] = le_family_b.transform(train_featured['model_b_family'])
test_featured['model_a_family_encoded'] = le_family_a.transform(test_featured['model_a_family'])
test_featured['model_b_family_encoded'] = le_family_b.transform(test_featured['model_b_family'])

print('âœ… ãƒ¢ãƒ‡ãƒ«ç‰¹å¾´é‡ã®è¿½åŠ å®Œäº†!')


# ### 4.3 TF-IDFç‰¹å¾´é‡

# In[13]:


print('ğŸ“ TF-IDFç‰¹å¾´é‡ã‚’ä½œæˆä¸­...')

tfidf_prompt = TfidfVectorizer(max_features=100, stop_words='english', ngram_range=(1, 2), min_df=5)
tfidf_response_a = TfidfVectorizer(max_features=100, stop_words='english', ngram_range=(1, 2), min_df=5)
tfidf_response_b = TfidfVectorizer(max_features=100, stop_words='english', ngram_range=(1, 2), min_df=5)

prompt_tfidf_train = tfidf_prompt.fit_transform(train_featured['prompt'].fillna(''))
prompt_tfidf_test = tfidf_prompt.transform(test_featured['prompt'].fillna(''))

response_a_tfidf_train = tfidf_response_a.fit_transform(train_featured['response_a'].fillna(''))
response_a_tfidf_test = tfidf_response_a.transform(test_featured['response_a'].fillna(''))

response_b_tfidf_train = tfidf_response_b.fit_transform(train_featured['response_b'].fillna(''))
response_b_tfidf_test = tfidf_response_b.transform(test_featured['response_b'].fillna(''))

print('âœ… TF-IDFç‰¹å¾´é‡ä½œæˆå®Œäº†!')


# In[14]:


# æœ€çµ‚ç‰¹å¾´é‡ã‚»ãƒƒãƒˆã®æº–å‚™
numerical_feature_cols = [
    'prompt_length', 'response_a_length', 'response_b_length',
    'prompt_words', 'response_a_words', 'response_b_words',
    'length_diff', 'length_ratio', 'words_diff', 'words_ratio',
    'avg_word_length_a', 'avg_word_length_b',
    'punctuation_a', 'punctuation_b', 'uppercase_a', 'uppercase_b',
    'digits_a', 'digits_b', 'newlines_a', 'newlines_b',
    'has_code_a', 'has_code_b', 'has_list_a', 'has_list_b',
    'model_a_win_rate', 'model_b_win_rate', 'win_rate_diff', 'same_family',
    'model_a_family_encoded', 'model_b_family_encoded'
]

X_train_num = train_featured[numerical_feature_cols].fillna(0)
X_test_num = test_featured[numerical_feature_cols].fillna(0)

X_train = hstack([csr_matrix(X_train_num.values), prompt_tfidf_train, response_a_tfidf_train, response_b_tfidf_train])
X_test = hstack([csr_matrix(X_test_num.values), prompt_tfidf_test, response_a_tfidf_test, response_b_tfidf_test])
y_train = train_featured[target_cols].values

print(f'\nâœ… æœ€çµ‚ç‰¹å¾´é‡ã‚»ãƒƒãƒˆå®Œæˆ!')
print(f'   ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿: {X_train.shape}')
print(f'   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {X_test.shape}')


# ---
# ## ğŸ¯ 5. ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
# 
# 4ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

# In[15]:


X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42, stratify=train_featured['winner']
)

print(f'ğŸ“Š ãƒ‡ãƒ¼ã‚¿åˆ†å‰²å®Œäº†: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° {X_train_split.shape[0]:,}ã€ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ {X_val_split.shape[0]:,}')


# In[16]:


# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°
print('\nğŸ”¨ ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...')
lr_model = MultiOutputClassifier(LogisticRegression(max_iter=1000, random_state=42, solver='saga', C=1.0))
lr_model.fit(X_train_split, y_train_split)
y_val_pred_lr = lr_model.predict_proba(X_val_split)
y_val_pred_lr_array = np.column_stack([pred[:, 1] for pred in y_val_pred_lr])
val_logloss_lr = log_loss(y_val_split, y_val_pred_lr_array)
print(f'âœ… ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸° - Log Loss: {val_logloss_lr:.4f}')


# In[17]:


# XGBoost
print('\nğŸš€ XGBoostã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...')
xgb_model = MultiOutputClassifier(xgb.XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42, tree_method='hist'))
xgb_model.fit(X_train_split, y_train_split)
y_val_pred_xgb = xgb_model.predict_proba(X_val_split)
y_val_pred_xgb_array = np.column_stack([pred[:, 1] for pred in y_val_pred_xgb])
val_logloss_xgb = log_loss(y_val_split, y_val_pred_xgb_array)
print(f'âœ… XGBoost - Log Loss: {val_logloss_xgb:.4f}')


# In[18]:


# LightGBM
print('\nâš¡ LightGBMã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...')
lgb_model = MultiOutputClassifier(lgb.LGBMClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42, verbose=-1))
lgb_model.fit(X_train_split, y_train_split)
y_val_pred_lgb = lgb_model.predict_proba(X_val_split)
y_val_pred_lgb_array = np.column_stack([pred[:, 1] for pred in y_val_pred_lgb])
val_logloss_lgb = log_loss(y_val_split, y_val_pred_lgb_array)
print(f'âœ… LightGBM - Log Loss: {val_logloss_lgb:.4f}')


# In[19]:


# ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ
print('\nğŸŒ² ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...')
rf_model = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1))
rf_model.fit(X_train_split, y_train_split)
y_val_pred_rf = rf_model.predict_proba(X_val_split)
y_val_pred_rf_array = np.column_stack([pred[:, 1] for pred in y_val_pred_rf])
val_logloss_rf = log_loss(y_val_split, y_val_pred_rf_array)
print(f'âœ… ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ - Log Loss: {val_logloss_rf:.4f}')


# ### 5.6 ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®æ¯”è¼ƒ

# In[20]:


model_comparison = pd.DataFrame({
    'ãƒ¢ãƒ‡ãƒ«': ['ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°', 'XGBoost', 'LightGBM', 'ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ'],
    'Log Loss': [val_logloss_lr, val_logloss_xgb, val_logloss_lgb, val_logloss_rf]
}).sort_values('Log Loss')

print('\nğŸ“Š ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ:')
display(model_comparison)

best_model_name = model_comparison.iloc[0]['ãƒ¢ãƒ‡ãƒ«']
best_logloss = model_comparison.iloc[0]['Log Loss']
print(f'\nğŸ† æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model_name} (Log Loss: {best_logloss:.4f})')


# In[21]:


# å¯è¦–åŒ–: ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ
fig = go.Figure()
colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']
fig.add_trace(go.Bar(
    x=model_comparison['ãƒ¢ãƒ‡ãƒ«'], y=model_comparison['Log Loss'],
    marker_color=colors,
    text=[f'{v:.4f}' for v in model_comparison['Log Loss']],
    textposition='outside'
))

fig.update_layout(
    title='<b>ãƒ¢ãƒ‡ãƒ«æ€§èƒ½æ¯”è¼ƒ (Log Loss)</b><br><sub>å€¤ãŒä½ã„ã»ã©è‰¯ã„</sub>',
    title_font_size=20, xaxis_title='ãƒ¢ãƒ‡ãƒ«', yaxis_title='Log Loss', height=500
)
fig.show()


# In[22]:


# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
print('\nğŸ¯ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬ã‚’ä½œæˆä¸­...')
y_val_pred_ensemble = (
    y_val_pred_lr_array * 0.2 + y_val_pred_xgb_array * 0.3 + 
    y_val_pred_lgb_array * 0.3 + y_val_pred_rf_array * 0.2
)
val_logloss_ensemble = log_loss(y_val_split, y_val_pred_ensemble)
print(f'âœ… ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ« - Log Loss: {val_logloss_ensemble:.4f}')

if val_logloss_ensemble < best_logloss:
    print('ğŸŠ ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ãŒæœ€è‰¯ã‚¹ã‚³ã‚¢ã‚’æ›´æ–°!')
    best_model_name = 'ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«'
    best_logloss = val_logloss_ensemble


# In[23]:


# å…¨ãƒ‡ãƒ¼ã‚¿ã§å†ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
print(f'\nğŸ¯ å…¨ãƒ‡ãƒ¼ã‚¿ã§å„ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­...')

lr_final = MultiOutputClassifier(LogisticRegression(max_iter=1000, random_state=42, solver='saga', C=1.0))
lr_final.fit(X_train, y_train)
print('âœ“ ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°')

xgb_final = MultiOutputClassifier(xgb.XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42, tree_method='hist'))
xgb_final.fit(X_train, y_train)
print('âœ“ XGBoost')

lgb_final = MultiOutputClassifier(lgb.LGBMClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, random_state=42, verbose=-1))
lgb_final.fit(X_train, y_train)
print('âœ“ LightGBM')

rf_final = MultiOutputClassifier(RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1))
rf_final.fit(X_train, y_train)
print('âœ“ ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ')

print('\nâœ… å…¨ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Œäº†!')


# ---
# ## ğŸ“¤ 6. äºˆæ¸¬ã¨æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ

# In[24]:


print('ğŸ”® ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ä¸­...')

y_test_pred_lr = lr_final.predict_proba(X_test)
y_test_pred_lr_array = np.column_stack([pred[:, 1] for pred in y_test_pred_lr])

y_test_pred_xgb = xgb_final.predict_proba(X_test)
y_test_pred_xgb_array = np.column_stack([pred[:, 1] for pred in y_test_pred_xgb])

y_test_pred_lgb = lgb_final.predict_proba(X_test)
y_test_pred_lgb_array = np.column_stack([pred[:, 1] for pred in y_test_pred_lgb])

y_test_pred_rf = rf_final.predict_proba(X_test)
y_test_pred_rf_array = np.column_stack([pred[:, 1] for pred in y_test_pred_rf])

# ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«äºˆæ¸¬
y_test_pred_array = (
    y_test_pred_lr_array * 0.2 + y_test_pred_xgb_array * 0.3 + 
    y_test_pred_lgb_array * 0.3 + y_test_pred_rf_array * 0.2
)

submission = pd.DataFrame({
    'id': test_df['id'],
    'winner_model_a': y_test_pred_array[:, 0],
    'winner_model_b': y_test_pred_array[:, 1],
    'winner_tie': y_test_pred_array[:, 2]
})

submission.to_csv('submission.csv', index=False)
print('\nğŸ’¾ æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ submission.csv ã¨ã—ã¦ä¿å­˜ã—ã¾ã—ãŸ!')
print(f'\nğŸŠ æœ€è‰¯ãƒ¢ãƒ‡ãƒ«: {best_model_name} (æ¤œè¨¼ Log Loss: {best_logloss:.4f})')
print('\nğŸ“¤ Kaggleã«æå‡ºã—ã¦çµæœã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†!')

display(submission.head())


# ---
# ## ğŸ“ ã¾ã¨ã‚
# 
# ### ğŸ¯ ã“ã®Notebookã§å®Ÿæ–½ã—ãŸã“ã¨
# 
# 1. **æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æ**
#    - ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†å¸ƒï¼ˆãƒãƒ©ãƒ³ã‚¹ãŒå–ã‚Œã¦ã„ã‚‹ï¼‰
#    - ãƒ†ã‚­ã‚¹ãƒˆé•·ã®åˆ†æï¼ˆé‡è¦ãªç‰¹å¾´é‡ï¼‰
#    - ãƒ¢ãƒ‡ãƒ«åˆ¥å‹ç‡ã®å¯è¦–åŒ–
#    - WordCloudã§é »å‡ºå˜èªã‚’ç¢ºèª
# 
# 2. **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**ï¼ˆ330ç‰¹å¾´é‡ï¼‰
#    - åŸºæœ¬çµ±è¨ˆç‰¹å¾´é‡ï¼ˆ26ï¼‰
#    - **ãƒ¢ãƒ‡ãƒ«æƒ…å ±ç‰¹å¾´é‡ï¼ˆ6ï¼‰** â† æœ€ã‚‚åŠ¹æœçš„ï¼
#    - TF-IDFç‰¹å¾´é‡ï¼ˆ300ï¼‰
# 
# 3. **ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰**
#    - ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›å¸°ï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰
#    - XGBoostï¼ˆæœ€è‰¯ãƒ¢ãƒ‡ãƒ«ï¼‰
#    - LightGBMï¼ˆ2ä½ï¼‰
#    - ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆ
#    - ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«
# 
# ### ğŸš€ ä»Šå¾Œã®æ”¹å–„æ¡ˆ
# 
# - Cross-Validationï¼ˆ5-fold CVï¼‰
# - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆOptunaï¼‰
# - ã‚ˆã‚Šé«˜åº¦ãªç‰¹å¾´é‡ï¼ˆã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆåˆ†æã€å¯èª­æ€§æŒ‡æ¨™ï¼‰
# - æ·±å±¤å­¦ç¿’ï¼ˆBERT/DeBERTaï¼‰
# 
# **ğŸŒŸ ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼**
